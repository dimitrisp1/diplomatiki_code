{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tovima.csv', encoding = 'utf-32',sep='\\t',na_values=\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"el_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens(tokens):\n",
    "    filtered_tokens = [\n",
    "        token for token in tokens\n",
    "        if not any(char.isalpha() and ord(char) < 128 for char in token)\n",
    "        and not all(char in string.punctuation for char in token)\n",
    "        and not token.isspace() and not token == '\\n'\n",
    "    ]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add(\"«\")\n",
    "nlp.Defaults.stop_words.add(\"»\")\n",
    "nlp.Defaults.stop_words.add(\"*\")\n",
    "nlp.Defaults.stop_words.add(\"”\")\n",
    "nlp.Defaults.stop_words.add(\"“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    tokens_without_stopwords = [token for token in tokens if token not in nlp.Defaults.stop_words]\n",
    "    return tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_key_words(tokens):\n",
    "    for i in range(len(tokens)):\n",
    "        word=tokens[i]\n",
    "        if word.lower() == \"eclass\" or word.lower() == \"zoom\" or word.lower == \"online\" or word.lower == \"on-line\":\n",
    "            tokens[i]=\"ψηφιακός\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_dates_and_times(tokens):\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    # Replace dates with 'ημερομηνία'\n",
    "    text = re.sub(r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', 'ημερομηνία', text)\n",
    "    text = re.sub(r'\\b\\d{1,2}-\\d{1,2}-\\d{2,4}\\b', 'ημερομηνία', text)\n",
    "    text = re.sub(r'\\b\\d{1,2}\\s(?:Ιανουαρίου|Φεβρουαρίου|Μαρτίου|Απριλίου|Μαΐου|Ιουνίου|Ιουλίου|Αυγούστου|Σεπτεμβρίου|Οκτωβρίου|Νοεμβρίου|Δεκεμβρίου)\\s\\d{2,4}\\b', 'ημερομηνία', text)\n",
    "    \n",
    "    # Replace times with 'ωρα'\n",
    "    text = re.sub(r'\\b\\d{1,2}:\\d{2}\\b', 'ώρα', text)\n",
    "    text = re.sub(r'\\b\\d{1,2}\\s(?:πμ|μμ)\\b', 'ώρα', text)\n",
    "    \n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numerics(text):\n",
    "    return ''.join(char for char in text if not char.isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemmatized_tokens = [token.lemma_ for token in tokens]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lowercase(tokens):\n",
    "    lowercase_tokens = [token.lower() for token in tokens]\n",
    "    return lowercase_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(tokens):\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_common_words(texts, num_common_words=10):\n",
    "    all_texts = \" \".join(texts)\n",
    "    words = all_texts.split()\n",
    "    word_counts = Counter(words)\n",
    "    most_common_words = word_counts.most_common(num_common_words)\n",
    "    return set([w for (w,wc) in most_common_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_freq(text):\n",
    "    return \" \".join([word for word in text.split() if word not in freq_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_letter_words(sentence):\n",
    "    words = sentence.split()\n",
    "    return ' '.join(word for word in words if len(word) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['Author']+ ' ' + df['Subject']+ ' ' +df['Message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14264/14264 [14:19<00:00, 16.59it/s]\n"
     ]
    }
   ],
   "source": [
    "df['tokens'] = df['text'].progress_apply(clean_and_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(replace_dates_and_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(replace_key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filtered_tokens'] = df['tokens'].apply(filter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens_without_stopwords'] = df['filtered_tokens'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14264/14264 [06:24<00:00, 37.11it/s]\n"
     ]
    }
   ],
   "source": [
    "df['lemmatized_tokens'] = df['tokens_without_stopwords'].progress_apply(lambda x: lemmatize(nlp(\" \".join(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_tokens']=df['lemmatized_tokens'].apply(convert_to_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['lemmatized_tokens'].apply(lambda x: len(x) > 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_sentences']=df['lemmatized_tokens'].apply(join_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words=print_most_common_words(df['lemmatized_sentences'], num_common_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['final_sentences']=df['lemmatized_sentences'].apply(remove_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['final_sentences']=df['final_sentences'].apply(remove_numerics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['final_sentences']=df['final_sentences'].apply(remove_single_letter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('preprocessed.csv', sep='\\t', encoding='utf-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
